{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91590d5d",
   "metadata": {},
   "source": [
    "# Lecture 2: Spark Dataframes\n",
    "\n",
    "Spark SQL is the Spark module dedicated to store and process structured datasets, e.g organized in columns such as Pandas Dataframes or Relational Databases tables. \n",
    "\n",
    "In Spark, structured datasets are referred to as Spark Dataframes, which anyway rely on the RDD way of storing and partitioning the data, but also provides richer optimizations under the hood.\n",
    "\n",
    "As for the previous lecture, select the most appropriate variable based on where this notebook is run. \n",
    "If the docker cluster is used, the process of starting spark cluster can be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this variable with one of the following values\n",
    "\n",
    "# -> 'local'\n",
    "# -> 'docker_container'\n",
    "# -> 'docker_cluster'\n",
    "\n",
    "CLUSTER_TYPE ='docker_cluster'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38914c8b",
   "metadata": {},
   "source": [
    "## Start the cluster \n",
    "\n",
    "Environment variables need to be set only in the case of a local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4d348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an enviromental variable\n",
    "\n",
    "%env CLUSTER_TYPE $CLUSTER_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLUSTER_TYPE=='local':\n",
    "    import findspark\n",
    "    findspark.init('/home/pazzini/work/courses/MAPD_B/MAPD-B/spark/spark-3.2.1-bin-hadoop3.2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    echo \"Launching master and worker\"\n",
    "    \n",
    "    # start master \n",
    "    $SPARK_HOME/sbin/start-master.sh --host localhost \\\n",
    "        --port 7077 --webui-port 8080\n",
    "    \n",
    "    # start worker\n",
    "    $SPARK_HOME/sbin/start-worker.sh spark://localhost:7077 \\\n",
    "        --cores 2 --memory 2g\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9907c1",
   "metadata": {},
   "source": [
    "## Create the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if CLUSTER_TYPE in ['local', 'docker_container']:\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://localhost:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "elif CLUSTER_TYPE == 'docker_cluster':\n",
    "    \n",
    "    # use the provided master\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .appName(\"First spark application\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beebfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d0dbef",
   "metadata": {},
   "source": [
    "## Creating a simple DataFrame\n",
    "\n",
    "Spark DataFrame is already very similar to the Pandas DataFrame API, and it can be created in a multitude of possible ways:\n",
    "\n",
    "- creating and appending a list of Rows (Spark objects for records) with an implicit schema definition\n",
    "- creating and appending a list of Python tuples with a explicit schema definition\n",
    "- importing a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f820ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6383baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a4973",
   "metadata": {},
   "source": [
    "As in the case of the RDD, the mere creation of the DataFrame does not imply any work is submitted to the executors.\n",
    "Whenever we ask Spark to retrieve something we instead trigger the execution of the jobs (as usual, subdivided in stages and tasks).\n",
    "\n",
    "To visualize the content of a Spark DataFrame we have similar interfaces to Pandas:\n",
    "- `DataFrame.show()` to visualize its content\n",
    "- `DataFrame.printSchema()` to visualize the schema of the structured dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c8db3",
   "metadata": {},
   "source": [
    "## Loading structured data\n",
    "\n",
    "By default Spark can create a DataFrame from data stored in many formats such as `csv`, `json` and many other listed [here](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html). \n",
    "\n",
    "If your dataset is stored in a format that Spark cannot understand natively, it is always possible to first create an RDD, and later convert the RDD into a DataFrame with the `toDF()` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ba4a26",
   "metadata": {},
   "source": [
    "Here we create some fake structured data in the form of a simple set of records with 2 features: `feature1` and `feature2`.\n",
    "\n",
    "We can parallelize the creation of the Rows in Spark as if we were loading and \"unpacking\" data from a number of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row is the equivalent of a record\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def read_custom_data(file_name):\n",
    "    # pretend to load some real data\n",
    "    #   this is some FAKE placeholder code, a placeholder for some \n",
    "    #   operation you might have to perform when reading the files\n",
    "    custom_data = []\n",
    "    for i in range(5):\n",
    "        event = {\n",
    "            'feature1': np.random.random(),\n",
    "            'feature2': np.random.random()\n",
    "        }\n",
    "        custom_data.append(Row(**event))\n",
    "    return custom_data\n",
    "    \n",
    "# read some files in parallel and create a dataframe\n",
    "file_list = ['file1', 'file2']\n",
    "\n",
    "rdd = sc.parallelize(file_list)\\\n",
    "        .flatMap(read_custom_data)\n",
    "\n",
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7076a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a collect() will always apply on the low-level Spark API, the RDD\n",
    "df.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a show() will instead \"wrap\" aroud the RDD to present you the output with the proper DataFrame schema\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffbf4c",
   "metadata": {},
   "source": [
    "## The `pyspark.pandas` API\n",
    "\n",
    "Very recently PySpark introduced a dedicated `Pandas` API on Spark, geared mainly for new users to make it even easier to move to and from the Pandas DataFrame and the PySpark DataFrame.\n",
    "\n",
    "We will mostly use the PySpark DataFrame in this notebook. You are suggested to take a look at the documentation on the Pandas API on Spark at the [link](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066bf1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c4f88",
   "metadata": {},
   "source": [
    "With the pyspark.pandas API the DataFrame creation is now streamlined and almost identical to plain Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad249e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045129e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bfb2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_pandas_df = ps.DataFrame(\n",
    "    {'a': [1, 2, 3, 4, 5, 6],\n",
    "     'b': [100, 200, 300, 400, 500, 600],\n",
    "     'c': [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]},\n",
    "    index=[10, 20, 30, 40, 50, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702518fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyspark_pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c716ba",
   "metadata": {},
   "source": [
    "Most of the common `Pandas` functions will still work on `pyspark.pandas`, so most probably a good fraction of the pure-Pandas code you have might as well work in Spark with little effort. \n",
    "\n",
    "**BUT**\n",
    "\n",
    "One also have to keep in mind that ***Pandas and PySpark are very different \"under the hood\"***.\n",
    "Where Pandas is single-thread and will host the DataFrame data in memory, PySpark is designed to host data into partitions using the executors' memory (or storage) and process it in a distributed way.\n",
    "\n",
    "This imply that many operations that are ideally very simple might be either impossible or simply very inefficient in PySpark.\n",
    "\n",
    "_For instance... how do you think the ordering of the entries in the table is handled when your dataframe is split into possibly hundreds of partitions?\n",
    "What if you want a dataframe to be sorted?_\n",
    "\n",
    "The data in a Spark dataframe does not preserve the natural order by default. \n",
    "We can force Spark to preserve the natural order but this causes a performance overhead with an additional stage of internal data sorting (a wide-depenency transformation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2f9e1",
   "metadata": {},
   "source": [
    "## Dimuon mass Example\n",
    "\n",
    "Several particles decay in a pair of opposite charged leptons (electrons, muons and taus).\n",
    "\n",
    "The dimuon spectrum, computed by calculating the invariant mass of muon pairs with opposite charge, features the presence of a number of narrow resonances, corresponding to the mass of the parent particle: from the η meson at about 548 MeV  up to the Z boson at about 91 GeV.\n",
    "\n",
    "Rare processes are also associated to this very same final state, such as the Bs dimuon decay (first observed in 2012 at CMS and LHCb), and the elusive Higgs dimuon decay (for which there is statistical evidence, but not yet an observation); new yet undiscovered particles might also show up as new resonances in the dimuon spectrum as the statistics and accelerator energy is increased.\n",
    "\n",
    "![Event Display](imgs/lecture2/event_display.png)\n",
    "\n",
    "The dataset used in this exercise is taken from the CERN Open Data portal (https://opendata.cern.ch/) and represents a portion of the data collected by the CMS collaboration at the Large Hadron Collider in 2010.\n",
    "\n",
    "This dataset comprise of only the fraction of events retained by online selections (trigger) which identify collisions where muons have been produced, thus storing only about 10 events out of the about 40 millions of collisions produced every second at LHC.\n",
    "\n",
    "The whole dataset collected by the CMS collaboration since the start of LHC operations in 2010 is comprised of tens of PBs of data and simulations.\n",
    "\n",
    "\n",
    "A subset of events have been extracted and preprocessed, the resulting files being stored as JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset on dataset/lecture2/dimuon\n",
    "#    spark.read --> read from a source of data\n",
    "#    format     --> choose between the available (semi)structured data formats\n",
    "#    load       --> the path to the input files\n",
    "df = spark.read \\\n",
    "    .option(\"inferTimestamp\",\"false\") \\\n",
    "    .option(\"prefersDecimal\",\"false\") \\\n",
    "    .format('json') \\\n",
    "    .load('../datasets/lecture2/dimuon/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41b580",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the first few (e.g. 5) rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of events per each sample\n",
    "# \n",
    "#   1. --> apply a .groupBy() transformation on the dataframe, grouping on the sample\n",
    "#   2. --> use a .count() aggregator \n",
    "#   3. --> show the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473375c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the dataframe into memory to avoid\n",
    "# loading it every time we want to use it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cdbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of muons\n",
    "# distribution and plot it\n",
    "# \n",
    "#   1. --> apply a .groupBy() transformation on the dataframe, grouping on the sample, and on the number of muons\n",
    "#   2. --> use a .count() aggregator \n",
    "#   3. --> sort by the number of muons\n",
    "#   4. --> collect the results\n",
    "\n",
    "num_muons_dist = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first few entries \n",
    "num_muons_dist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228a7a7",
   "metadata": {},
   "source": [
    "Plotting this data from Spark alone would require a lot of code, but fortunately there is a trick we can use: it is possible to convert a Spark dataframe into a Pandas Dataframe with `toPandas()`. \n",
    "\n",
    "Again, as it happens for `collect()`, the entire resulting dataset is fetched into the master and it may not fit in the memory, so do not use `toPandas()` very lightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24545648",
   "metadata": {},
   "source": [
    "(**Side Note on Apache Arrow**)\n",
    "_Apache Arrow comes into play in this contex: it is an in-memory columnar data format that is used in spark to efficiently transfer data between the JVM and Python processes. When it is not enabled Spark comunicates with python processes by serializing and deserializing one element of the time. With Arrow this operation is \"vectorized\", i.e. one column at the time is transfered. This is possible because both spark and pandas included Arrow representation._\n",
    "\n",
    "_[Here](https://xuechendi.github.io/2019/04/16/Apache-Arrow) you can find a nice blog post explaining how it works._\n",
    "\n",
    "_In this way, operations between PySpark and Pandas are more efficient and we can try to get the best from both worlds._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ee0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# run the same dataframe operation as before, \n",
    "# but instead of collecting the results, \n",
    "# put the results into a pandas dataframe using the .toPandas() function\n",
    "num_muons_dist = \n",
    "\n",
    "num_muons_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569dada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the type of this object \n",
    "# to be sure this is now a real pandas DataFrame\n",
    "type(num_muons_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac386f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the histogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "mc_counts = num_muons_dist.loc[num_muons_dist['sample']=='mc']\n",
    "plt.bar(\n",
    "    mc_counts['nMuon'], mc_counts['count'],\n",
    "    label = 'Simulation', width=1, alpha=0.6\n",
    ")\n",
    "\n",
    "data_counts = num_muons_dist.loc[num_muons_dist['sample']=='data']\n",
    "plt.errorbar(\n",
    "    data_counts['nMuon'],data_counts['count'],\n",
    "    yerr = np.sqrt(data_counts['count']),\n",
    "    label = 'Data', color='black', fmt='o'\n",
    ")\n",
    "plt.xlim(-0.5,16.5)\n",
    "plt.xlabel(\"Number of muons\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdc9ac",
   "metadata": {},
   "source": [
    "We are interested in dimuon events, i.e. events having 2 muons. \n",
    "\n",
    "We can do this with:\n",
    "- \"plain dataframe\"-like selections, e.g. `df[df.column > xyz]`\n",
    "- `filter` \n",
    "- `where`\n",
    "- SQL-like statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea2d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only events with two muons in a \"plain dataframe\" style\n",
    "df[df['nMuon']==2].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad103e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# use a where statement on the column objects from spark\n",
    "df.where(col('nMuon')==2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d815fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a where statement on the content of the dataframe\n",
    "dimuon_df = df.where(df['nMuon']==2)\n",
    "dimuon_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a filter statement on the content of the dataframe\n",
    "df.filter(df['nMuon']==2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66049d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a SQL-like view (a \"table\") of the records \n",
    "df.createOrReplaceTempView(\"dimuon_df_table\")\n",
    "\n",
    "# use plain SQL to filter the events with a SELECT statement\n",
    "sqlDF = spark.sql(\"SELECT * FROM dimuon_df_table WHERE nMuon = 2\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c4009",
   "metadata": {},
   "source": [
    "At this stage the Dataset is not flat (we have nested content in the Muons column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is not flat\n",
    "dimuon_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a9cf9",
   "metadata": {},
   "source": [
    "Now we can make our dataset flat for further processing: in this way we will be able to perform operations between the columns.\n",
    "\n",
    "A new column can be created by using `withColumn(name, func)` where `func` is a function taking as input one or more columns. An arbitrary complex function can be used to produce the new column: for example we can define a **User Defined Function (UDF)** which will be applied to every row of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c202548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns for the two muons\n",
    "\n",
    "# select()     projects a set of expressions and returns a new DataFrame\n",
    "# withColumn() creates a new table attribute from the given column\n",
    "# drop()       removes columns from the table\n",
    "dimuon_flat = (\n",
    "    # keep only sample/nMuon/Muons[0]/Muons[1] from the table\n",
    "    dimuon_df.select([col('sample'), col('nMuon'), col('Muons')[0], col('Muons')[1]]) \n",
    "    # create a number of additional columns from the Muons[0/1] nested attributes\n",
    "    .withColumn('E1', col('Muons[0].E'))\n",
    "    .withColumn('px1', col('Muons[0].px'))\n",
    "    .withColumn('py1', col('Muons[0].py'))\n",
    "    .withColumn('pz1', col('Muons[0].pz'))\n",
    "    .withColumn('c1',  col('Muons[0].charge'))\n",
    "    .withColumn('E2',  col('Muons[1].E'))\n",
    "    .withColumn('px2', col('Muons[1].px'))\n",
    "    .withColumn('py2', col('Muons[1].py'))\n",
    "    .withColumn('pz2', col('Muons[1].pz'))\n",
    "    .withColumn('c2',  col('Muons[1].charge'))\n",
    "    # remove the original Muons[0] and Muons[1] nested attributes\n",
    "    .drop('Muons[0]', 'Muons[1]')\n",
    ")\n",
    "\n",
    "dimuon_flat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f5b04",
   "metadata": {},
   "source": [
    "We can now continue with our work and select only the events where the two muons have opposite charge (column `c1` and `c2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select candidates with opposite charge muons\n",
    "dimuon_flat = \n",
    "dimuon_flat.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76711812",
   "metadata": {},
   "source": [
    "To compute the invariant mass of the two candidate muons we need first to compute the four-momentum of the parent particle \n",
    "\n",
    "$$\n",
    "p = \\left(E_1+E_2,\\quad p_{x,1}+p_{x,2},\\quad p_{y,1}+p_{y,2},\\quad p_{z,1}+p_{z,2}\\right) = (E,\\, \\mathbf{p})\n",
    "$$\n",
    "\n",
    "Create four new columns in the dataframe, one per each coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f541cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns for the four momentum\n",
    "dimuon_flat = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367177bd",
   "metadata": {},
   "source": [
    "From the 4-momentum of the candidate we can compute the invariant mass as\n",
    "\n",
    "$$\n",
    "M = \\sqrt{p\\cdot p} =  \\sqrt{(E^2 - \\|\\mathbf{p}\\|^2)} = \\sqrt{(E^2 - (px^2 + py^2 + pz^2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute invariant mass\n",
    "from pyspark.sql.functions import sqrt\n",
    "\n",
    "dimuon_flat = \n",
    "\n",
    "# persist the dataframe in memory\n",
    "dimuon_flat = \n",
    "\n",
    "# show the first 5 elements of the mass (M) attribute\n",
    "dimuon_flat.select('M').show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f3cab",
   "metadata": {},
   "source": [
    "It is possible to obtain the same result using a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92205706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import math\n",
    "\n",
    "# obtain the same result using udf\n",
    "#    create a UDF that takes as inputs E, px, py, pz and returns the invariant mass\n",
    "@udf('float')\n",
    "def invariant_mass(\"\"\" --- \"\"\"):\n",
    "    return \"\"\" --- \"\"\"\n",
    "\n",
    "# apply the UDF by simply calling it with the proper columns inputs (e.g. col('E') )\n",
    "dimuon_flat.select(\n",
    "    invariant_mass( \"\"\" --- \"\"\" )\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34c75a",
   "metadata": {},
   "source": [
    "Similarly, we can use **Pandas UDFs**. \n",
    "\n",
    "This allows to process pyspark's dataframes as Pandas dataframes and Pandas series.\n",
    "\n",
    "They result particularly efficient in the grouped maps, i.e. by appling a function after a groupby. \n",
    "\n",
    "More on this can be found [here](https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html#grouped-map). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4590b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "# obtain the same result using a pandas udf\n",
    "#    create a UDF that takes as inputs the Pandas Series for E, px, py, pz \n",
    "#    and returns the Pandas Series for the invariant mass\n",
    "@pandas_udf('float')\n",
    "def invariant_mass_udf(\n",
    "    E: pd.Series,\n",
    "    \"\"\" ... \"\"\"\n",
    ") -> pd.Series:\n",
    "    return \"\"\" --- \"\"\"\n",
    "\n",
    "# apply the Pandas UDF \n",
    "#   rename the output by using the .alias(\"new name\") method\n",
    "dimuon_flat.select(\n",
    "    invariant_mass_udf( \"\"\" --- \"\"\").alias( \"\"\" --- \"\"\")\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75b03f",
   "metadata": {},
   "source": [
    "Compute the mean energy for different groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f38c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean energy for each sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the same results with a Pandas UDF \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d783a67",
   "metadata": {},
   "source": [
    "## (Re-)Discovering particles\n",
    "\n",
    "Now that all the interesting quantities have been computed we can use them to perform the analysis.\n",
    "\n",
    "For example, let's plot the invariant mass of the dimuon system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the histogram of M\n",
    "bins, counts = dimuon_flat.select('M') \\\n",
    "                          .rdd \\\n",
    "                          .map(lambda x: x.M) \\\n",
    "                          .histogram(list(np.arange(0,115,0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd4905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute it for each sample\n",
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    \"\"\" --- \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71712c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Montecarlo', alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "\n",
    "plt.errorbar(\n",
    "    bin_centers, data_res['counts'], yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', ms=4, lw=1, color='black', label='data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1892c",
   "metadata": {},
   "source": [
    "In the range 70-120 GeV it appears there is a large resonance, we can histogram the data in that range to see it better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_results = {}\n",
    "for sample in ['mc', 'data']:\n",
    "    histogram_results[sample] = {}\n",
    "    \n",
    "    # evaluate the new histograms here in the range 80-100\n",
    "    bins, counts = \"\"\" --- \"\"\"\n",
    "    \n",
    "    histogram_results[sample]['bins'] = bins\n",
    "    histogram_results[sample]['counts'] = counts\n",
    "    \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = histogram_results['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Montecarlo', alpha=0.6\n",
    ")\n",
    "\n",
    "data_res = histogram_results['data']\n",
    "bin_centers = data_res['bins'][:-1] + np.diff(data_res['bins'])/2\n",
    "\n",
    "plt.errorbar(\n",
    "    bin_centers, data_res['counts'], yerr=np.sqrt(data_res['counts']),\n",
    "    fmt='o', ms=4, lw=1, color='black', label='data'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "#plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a7328",
   "metadata": {},
   "source": [
    "We can try to perform some selection, to improve the quality of the signal and remove background. To do this we can try look at candidates with a large transverse momentum \n",
    "\n",
    "$$\n",
    "p_T = \\sqrt{p_x^2 + p_y^2}\n",
    "$$\n",
    "\n",
    "create a colum with name \"energetic\" and for each row set the value to `True` if $p_T\\geq 30$ Gev. Plot then the results and see if there is any improvement. You can compare just the data or montecarlo. \n",
    "\n",
    "This can be achieved in spark using `when()` and `otherwise()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba342cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a udf to evaluate the dimuon candidate pT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591ecb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "dimuon_flat = dimuon_flat.withColumn(\"highpt\", when(dimuon_flat.highpt>30,True).otherwise(False) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb08d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimuon_flat['sample','M','highpt'].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d446d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_histogram(dataset, column):\n",
    "    histogram_results = {}\n",
    "    for sample in ['mc', 'data']:\n",
    "        histogram_results[sample] = {}\n",
    "\n",
    "        # create the invariant mass histogram in the range 70-115\n",
    "        bins, counts = \"\"\" --- \"\"\"\n",
    "\n",
    "        histogram_results[sample]['bins'] = bins\n",
    "        histogram_results[sample]['counts'] = counts\n",
    "        \n",
    "    return histogram_results\n",
    "        \n",
    "# compute two histograms:\n",
    "#   - one for the energetic (highpt) cadidates\n",
    "#   - another one for the non-energetic (not highpt) cadidates\n",
    "is_energetic = \"\"\" --- \"\"\"\n",
    "not_energetic = \"\"\" --- \"\"\"\n",
    " \n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "mc_res = not_energetic['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Not energetic', alpha=0.6\n",
    ")\n",
    "\n",
    "mc_res = is_energetic['mc']\n",
    "bin_centers = mc_res['bins'][:-1] + np.diff(mc_res['bins'])/2\n",
    "plt.hist(\n",
    "    bin_centers, weights=mc_res['counts'], bins=mc_res['bins'],\n",
    "    label='Energetic', alpha=0.6\n",
    ")\n",
    "\n",
    "plt.xlabel(\"$m_{\\mu \\mu}$ [GeV]\")\n",
    "plt.ylabel(\"Events/0.5GeV\")\n",
    "plt.legend()\n",
    "#plt.semilogy()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53cb88c",
   "metadata": {},
   "source": [
    "## Stop worker and master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ffaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a5906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script bash --no-raise-error\n",
    "\n",
    "if [[ \"$CLUSTER_TYPE\" != \"docker_cluster\" ]]; then\n",
    "    # stop worker \n",
    "    $SPARK_HOME/sbin/stop-worker.sh\n",
    "    \n",
    "    # start master\n",
    "    $SPARK_HOME/sbin/stop-master.sh\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15eac15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
